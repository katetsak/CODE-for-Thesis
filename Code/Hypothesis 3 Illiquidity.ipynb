{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import wilcoxon, mannwhitneyu\n",
    "from statsmodels.stats.descriptivestats import sign_test\n",
    "\n",
    "def calculate_amihud_illiquidity(df):\n",
    "   \n",
    "    df['return'] = pd.to_numeric(df['return'], errors='coerce')\n",
    "    df['volume'] = pd.to_numeric(df['volume'], errors='coerce')\n",
    "    \n",
    "    df = df.dropna(subset=['return', 'volume'])\n",
    "    df.loc[:, 'abs_return'] = df['return'].abs()\n",
    "    df.loc[:, 'amihud_illiq'] = df['abs_return'] / df['volume']\n",
    "    df = df.dropna(subset=['amihud_illiq'])\n",
    "    \n",
    "    #group by month and calculate mean illiquidity\n",
    "    illiquidity = df.groupby(df['date'].dt.to_period('M'))['amihud_illiq'].mean()\n",
    "    return illiquidity\n",
    "\n",
    "def process_excel_file(file_path):\n",
    "    xls = pd.ExcelFile(file_path)\n",
    "    results = {}\n",
    "    for sheet_name in xls.sheet_names:\n",
    "        df = pd.read_excel(xls, sheet_name)\n",
    "        if df.empty:\n",
    "            continue  # Skip empty sheets\n",
    "        df.columns = df.columns.str.strip() \n",
    "        if 'date' not in df.columns or 'return' not in df.columns or 'volume' not in df.columns:\n",
    "            continue  \n",
    "        df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "        df.dropna(subset=['date'], inplace=True)\n",
    "        results[sheet_name] = calculate_amihud_illiquidity(df)\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "file_path = r\"C:\\UU\\THESIS\\AMX\\HYPOTHESIS3\\DELETED_AMIHOUD.xlsx\"\n",
    "amihud_illiquidity_df = process_excel_file(file_path)\n",
    "\n",
    "amihud_illiquidity_df.to_excel(r\"C:\\UU\\THESIS\\AMX\\HYPOTHESIS3\\DELETED_amihud_illiquidity_results.xlsx\", index=True)\n",
    "\n",
    "print(amihud_illiquidity_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "file_path = r\"C:\\UU\\THESIS\\AMX\\HYPOTHESIS3\\AMIHOUD\\DELETED_ILLIQUIDITY.xlsx\"\n",
    "output_path = r\"C:\\UU\\THESIS\\AMX\\HYPOTHESIS3\\AMIHOUD\\DELETED_yearly_illiquidity.xlsx\"\n",
    "\n",
    "sheets = pd.read_excel(file_path, sheet_name=None)\n",
    "\n",
    "yearly_illiquidity_data = {}\n",
    "\n",
    "for sheet_name, df in sheets.items():\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df.set_index('date', inplace=True)\n",
    "\n",
    "    yearly_mean = df.resample('Y').mean()\n",
    "    yearly_mean['year'] = yearly_mean.index.year\n",
    "    yearly_illiquidity_data[sheet_name] = yearly_mean.reset_index(drop=True)\n",
    "\n",
    "\n",
    "all_yearly_data = pd.concat(yearly_illiquidity_data, axis=1)\n",
    "\n",
    "\n",
    "with pd.ExcelWriter(output_path) as writer:\n",
    "    for company, data in yearly_illiquidity_data.items():\n",
    "        data.to_excel(writer, sheet_name=company)\n",
    "\n",
    "print(f\"Yearly illiquidity data saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import mannwhitneyu, wilcoxon, binomtest, ttest_rel\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "file_path = r\"C:\\UU\\THESIS\\AMX\\HYPOTHESIS3\\AMIHOUD\\deleted yearly.xlsx\"\n",
    "xls = pd.ExcelFile(file_path)\n",
    "data = pd.concat([pd.read_excel(xls, sheet_name=sheet) for sheet in xls.sheet_names])\n",
    "\n",
    "\n",
    "periods_deleted = {\n",
    "    't-5': 0,\n",
    "    't': 5,\n",
    "    't+5': 10\n",
    "}\n",
    "\n",
    "\n",
    "df_t_minus_5 = data[data['event'] == periods_deleted['t-5']]\n",
    "df_t = data[data['event'] == periods_deleted['t']]\n",
    "df_t_plus_5 = data[data['event'] == periods_deleted['t+5']]\n",
    "\n",
    "\n",
    "df_t_minus_5 = df_t_minus_5.sort_values(by='event').reset_index(drop=True)\n",
    "df_t = df_t.sort_values(by='event').reset_index(drop=True)\n",
    "df_t_plus_5 = df_t_plus_5.sort_values(by='event').reset_index(drop=True)\n",
    "\n",
    "#define the columns to test\n",
    "columns_to_test = ['company']\n",
    "\n",
    "\n",
    "results = {\n",
    "    'mann_whitney': {'Measure': [], 'Period': [], 'Statistic': [], 'p-value': []},\n",
    "    'wilcoxon': {'Measure': [], 'Period': [], 'Statistic': [], 't-value': [], 'p-value': []},\n",
    "    'sign_test': {'Measure': [], 'Period': [], 'Statistic': [], 'p-value': []},\n",
    "    'paired_ttest': {'Measure': [], 'Period': [], 'Statistic': [], 'p-value': []}\n",
    "}\n",
    "\n",
    "def perform_wilcoxon_test(df1, df2, column):\n",
    "    data1 = df1[column].dropna().values\n",
    "    data2 = df2[column].dropna().values\n",
    "    if len(data1) == 0 or len(data2) == 0:\n",
    "        return (None, None, None)\n",
    "    \n",
    "    stat, p_value = wilcoxon(data1, data2)\n",
    "    \n",
    "    #calculate the t-value from the Wilcoxon statistic\n",
    "    n = len(data1)\n",
    "    mu_w = n * (n + 1) / 4\n",
    "    sigma_w = np.sqrt(n * (n + 1) * (2 * n + 1) / 24)\n",
    "    t_value = (stat - mu_w) / sigma_w\n",
    "    \n",
    "    return stat, t_value, p_value\n",
    "\n",
    "\n",
    "def perform_sign_test(df1, df2, column):\n",
    "    data1 = df1[column].dropna().values\n",
    "    data2 = df2[column].dropna().values\n",
    "    min_length = min(len(data1), len(data2))\n",
    "    data1 = data1[:min_length]\n",
    "    data2 = data2[:min_length]\n",
    "    differences = data1 - data2\n",
    "    n_positive = sum(differences > 0)\n",
    "    n_negative = sum(differences < 0)\n",
    "    n_total = n_positive + n_negative\n",
    "    if n_total == 0:\n",
    "        return (None, None)\n",
    "    p_value = binomtest(n_positive, n_total, 0.5, alternative='two-sided').pvalue\n",
    "    return (n_positive - n_negative, p_value)\n",
    "\n",
    "\n",
    "for column in columns_to_test:\n",
    "    #mann-Whitney U Test\n",
    "    if not df_t_minus_5[column].dropna().empty and not df_t[column].dropna().empty:\n",
    "        stat, p_value = mannwhitneyu(df_t_minus_5[column], df_t[column], alternative='two-sided')\n",
    "        results['mann_whitney']['Measure'].append(column)\n",
    "        results['mann_whitney']['Period'].append('t-5 vs t')\n",
    "        results['mann_whitney']['Statistic'].append(stat)\n",
    "        results['mann_whitney']['p-value'].append(p_value)\n",
    "        \n",
    "    if not df_t[column].dropna().empty and not df_t_plus_5[column].dropna().empty:\n",
    "        stat, p_value = mannwhitneyu(df_t[column], df_t_plus_5[column], alternative='two-sided')\n",
    "        results['mann_whitney']['Measure'].append(column)\n",
    "        results['mann_whitney']['Period'].append('t vs t+5')\n",
    "        results['mann_whitney']['Statistic'].append(stat)\n",
    "        results['mann_whitney']['p-value'].append(p_value)\n",
    "\n",
    "    #Wilcoxon Signed Rank Test\n",
    "    stat, t_value, p_value = perform_wilcoxon_test(df_t_minus_5, df_t, column)\n",
    "    results['wilcoxon']['Measure'].append(column)\n",
    "    results['wilcoxon']['Period'].append('t-5 vs t')\n",
    "    results['wilcoxon']['Statistic'].append(stat)\n",
    "    results['wilcoxon']['t-value'].append(t_value)\n",
    "    results['wilcoxon']['p-value'].append(p_value)\n",
    "    \n",
    "    stat, t_value, p_value = perform_wilcoxon_test(df_t, df_t_plus_5, column)\n",
    "    results['wilcoxon']['Measure'].append(column)\n",
    "    results['wilcoxon']['Period'].append('t vs t+5')\n",
    "    results['wilcoxon']['Statistic'].append(stat)\n",
    "    results['wilcoxon']['t-value'].append(t_value)\n",
    "    results['wilcoxon']['p-value'].append(p_value)\n",
    "\n",
    "    #sign Test\n",
    "    stat, p_value = perform_sign_test(df_t_minus_5, df_t, column)\n",
    "    results['sign_test']['Measure'].append(column)\n",
    "    results['sign_test']['Period'].append('t-5 vs t')\n",
    "    results['sign_test']['Statistic'].append(stat)\n",
    "    results['sign_test']['p-value'].append(p_value)\n",
    "    \n",
    "    stat, p_value = perform_sign_test(df_t, df_t_plus_5, column)\n",
    "    results['sign_test']['Measure'].append(column)\n",
    "    results['sign_test']['Period'].append('t vs t+5')\n",
    "    results['sign_test']['Statistic'].append(stat)\n",
    "    results['sign_test']['p-value'].append(p_value)\n",
    "    \n",
    "    #paired T-Test\n",
    "    data_t_minus_5 = df_t_minus_5[column].dropna().values\n",
    "    data_t = df_t[column].dropna().values\n",
    "    data_t_plus_5 = df_t_plus_5[column].dropna().values\n",
    "    min_length_t_minus_5_t = min(len(data_t_minus_5), len(data_t))\n",
    "    min_length_t_t_plus_5 = min(len(data_t), len(data_t_plus_5))\n",
    "    data_t_minus_5 = data_t_minus_5[:min_length_t_minus_5_t]\n",
    "    data_t = data_t[:min_length_t_minus_5_t]\n",
    "    data_t_plus_5 = data_t_plus_5[:min_length_t_t_plus_5]\n",
    "    if len(data_t_minus_5) > 1 and len(data_t) > 1:\n",
    "        stat, p_value = ttest_rel(data_t_minus_5, data_t)\n",
    "        results['paired_ttest']['Measure'].append(column)\n",
    "        results['paired_ttest']['Period'].append('t-5 vs t')\n",
    "        results['paired_ttest']['Statistic'].append(stat)\n",
    "        results['paired_ttest']['p-value'].append(p_value)\n",
    "    else:\n",
    "        results['paired_ttest']['Measure'].append(column)\n",
    "        results['paired_ttest']['Period'].append('t-5 vs t')\n",
    "        results['paired_ttest']['Statistic'].append(None)\n",
    "        results['paired_ttest']['p-value'].append(None)\n",
    "    if len(data_t) > 1 and len(data_t_plus_5) > 1:\n",
    "        stat, p_value = ttest_rel(data_t, data_t_plus_5)\n",
    "        results['paired_ttest']['Measure'].append(column)\n",
    "        results['paired_ttest']['Period'].append('t vs t+5')\n",
    "        results['paired_ttest']['Statistic'].append(stat)\n",
    "        results['paired_ttest']['p-value'].append(p_value)\n",
    "    else:\n",
    "        results['paired_ttest']['Measure'].append(column)\n",
    "        results['paired_ttest']['Period'].append('t vs t+5')\n",
    "        results['paired_ttest']['Statistic'].append(None)\n",
    "        results['paired_ttest']['p-value'].append(None)\n",
    "\n",
    "results_mann_whitney_df = pd.DataFrame(results['mann_whitney'])\n",
    "results_wilcoxon_df = pd.DataFrame(results['wilcoxon'])\n",
    "results_sign_test_df = pd.DataFrame(results['sign_test'])\n",
    "results_paired_ttest_df = pd.DataFrame(results['paired_ttest'])\n",
    "\n",
    "\n",
    "print(results_mann_whitney_df)\n",
    "\n",
    "print(results_wilcoxon_df)\n",
    "\n",
    "print(results_sign_test_df)\n",
    "\n",
    "print(results_paired_ttest_df)\n",
    "\n",
    "\n",
    "output_path = r\"C:\\UU\\THESIS\\AMX\\HYPOTHESIS3\\AMIHOUD\\DELETED_significance_test_results_updated.xlsx\"\n",
    "with pd.ExcelWriter(output_path) as writer:\n",
    "    results_mann_whitney_df.to_excel(writer, sheet_name='Mann-Whitney')\n",
    "    results_wilcoxon_df.to_excel(writer, sheet_name='Wilcoxon')\n",
    "    results_sign_test_df.to_excel(writer, sheet_name='Sign Test')\n",
    "    results_paired_ttest_df.to_excel(writer, sheet_name='Paired T-Test')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "file_path = r\"C:\\UU\\THESIS\\AMX\\HYPOTHESIS3\\AMIHOUD\\yearly_illiquidity.xlsx\"\n",
    "sheets = pd.read_excel(file_path, sheet_name=None)\n",
    "aggregated_results = pd.DataFrame(columns=['event', 'company'])\n",
    "\n",
    "for sheet_name, df in sheets.items():\n",
    "    if df.empty:\n",
    "        print(f\"Sheet {sheet_name} is empty, skipping.\")\n",
    "        continue  # Skip empty sheets\n",
    "    if 'company' not in df.columns or 'event' not in df.columns:\n",
    "        continue\n",
    "    \n",
    "    df['company'] = pd.to_numeric(df['company'], errors='coerce')\n",
    "    \n",
    "\n",
    "    #group by event year and calculate the mean for coverage and dispersion\n",
    "    mean_proxies = df.groupby('event')[['company']].mean()\n",
    "\n",
    "    \n",
    "    mean_proxies.reset_index(inplace=True)\n",
    "    aggregated_results = pd.concat([aggregated_results, mean_proxies], ignore_index=True)\n",
    "\n",
    "#group by event year again to get the overall mean across all companies\n",
    "final_mean_proxies = aggregated_results.groupby('event').mean()\n",
    "\n",
    "output_path = r\"C:\\UU\\THESIS\\AMX\\HYPOTHESIS3\\AMIHOUD\\ADDED_average_proxies_by_event_year.xlsx\"\n",
    "final_mean_proxies.to_excel(output_path)\n",
    "\n",
    "print(\"Average proxies by event year saved to:\", output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
